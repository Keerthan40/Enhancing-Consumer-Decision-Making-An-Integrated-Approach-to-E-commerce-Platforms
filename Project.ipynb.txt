{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdilZInbawzz",
        "outputId": "74fb141c-d6af-4bb2-f3ec-275c2570b785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.amazon.in/s?k=Galaxy+S23&page=1&ref=nb_sb_noss\n",
            "Scraping next page: https://www.amazon.in/s?k=Galaxy+S23&page=1&ref=nb_sb_noss\n",
            "https://www.amazon.in/s?k=Galaxy+S23&page=2&ref=nb_sb_noss\n",
            "Scraping next page: https://www.amazon.in/s?k=Galaxy+S23&page=2&ref=nb_sb_noss\n",
            "Scraping completed. Collected 20 results. Check amazon_products.csv file for results.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "def scrape_amazon(search_query, max_results):\n",
        "    headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.0.0 Safari/537.36\",\n",
        "    }\n",
        "\n",
        "    products_count = 0\n",
        "    page_num = 1\n",
        "\n",
        "    with open('amazon_products.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['Product Name', 'Image URL', 'Product URL', 'Price', 'Rating', 'Rating Count']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        while products_count < max_results:\n",
        "            base_url = f\"https://www.amazon.in/s?k={search_query.replace(' ', '+')}&page={page_num}&ref=nb_sb_noss\"\n",
        "            current_page = base_url\n",
        "            print(current_page)\n",
        "            response = requests.get(current_page, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "                products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
        "\n",
        "                for product in products:\n",
        "                    product_name = product.find('span', {'class': 'a-text-normal'}).text.strip()\n",
        "\n",
        "                    image_tag = product.find('img', {'class': 's-image'})\n",
        "                    image_url = image_tag['src'] if image_tag else None\n",
        "\n",
        "                    product_url = 'https://www.amazon.in' + product.find('a', {'class': 'a-link-normal'})['href']\n",
        "\n",
        "                    price = product.find('span', {'class': 'a-price-whole'})\n",
        "                    price = price.text.strip() if price else None\n",
        "\n",
        "                    rating = product.find('span', {'class': 'a-icon-alt'})\n",
        "                    rating = rating.text.split()[0] if rating else None\n",
        "\n",
        "                    rating_count = product.find('span', {'class': 'a-size-base'})\n",
        "                    rating_count = rating_count.text if rating_count else None\n",
        "\n",
        "                    writer.writerow({\n",
        "                        'Product Name': product_name,\n",
        "                        'Image URL': image_url,\n",
        "                        'Product URL': product_url,\n",
        "                        'Price': price,\n",
        "                        'Rating': rating,\n",
        "                        'Rating Count': rating_count\n",
        "                    })\n",
        "                    products_count += 1\n",
        "                    if products_count >= max_results:\n",
        "                        break\n",
        "\n",
        "                next_page_link = base_url\n",
        "\n",
        "                if not next_page_link:\n",
        "                    print(\"No more pages available.\")\n",
        "                    break\n",
        "\n",
        "                current_page = next_page_link\n",
        "                print(f\"Scraping next page: {current_page}\")\n",
        "                time.sleep(20)  # Adding a delay to avoid overwhelming the server\n",
        "            else:\n",
        "                print('Failed to fetch the page.')\n",
        "                break\n",
        "\n",
        "            page_num += 1\n",
        "\n",
        "    print(f'Scraping completed. Collected {products_count} results. Check amazon_products.csv file for results.')\n",
        "\n",
        "# Replace 'your search keyword' with the search term you want to use\n",
        "search_term = 'Galaxy S23'\n",
        "max_results_to_collect = 20  # Set the maximum number of results to collect\n",
        "scrape_amazon(search_term, max_results_to_collect)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "def scrape_reviews_to_csv(product_url, row):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4472.130 Safari/537.39'\n",
        "    }\n",
        "\n",
        "    response = requests.get(product_url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        product_title = soup.find('span', {'id': 'productTitle'})\n",
        "        if product_title:\n",
        "            product_title = product_title.get_text(strip=True)\n",
        "\n",
        "            reviews = []\n",
        "            review_blocks = soup.find_all('div', {'data-hook': 'review'})\n",
        "            for review_block in review_blocks:\n",
        "                user_name = review_block.find('span', {'class': 'a-profile-name'})\n",
        "                user_name = user_name.get_text(strip=True) if user_name else None\n",
        "\n",
        "                review_date = review_block.find('span', {'data-hook': 'review-date'})\n",
        "                review_date = review_date.get_text(strip=True) if review_date else None\n",
        "\n",
        "                review_title = review_block.find('a', {'data-hook': 'review-title'})\n",
        "                review_title = review_title.get_text(strip=True) if review_title else None\n",
        "\n",
        "                review_content = review_block.find('span', {'data-hook': 'review-body'})\n",
        "                review_content = review_content.get_text(strip=True) if review_content else None\n",
        "\n",
        "                rating = review_block.find('i', {'data-hook': 'review-star-rating'})\n",
        "                rating = rating.get_text(strip=True) if rating else None\n",
        "\n",
        "                # Assume review_size is the length of the review content\n",
        "                review_size = len(review_content) if review_content else None\n",
        "\n",
        "                reviews.append({\n",
        "                    'Product URL': product_url,\n",
        "                    'User name': user_name,\n",
        "                    'Rating': rating,\n",
        "                    'Review_date': review_date,\n",
        "                    'Review_title': review_title,\n",
        "                    'Review_content': review_content,\n",
        "                    'Review_size': review_size\n",
        "                })\n",
        "\n",
        "            # Create a CSV file for each product\n",
        "            csv_filename = f\"{product_title.replace(' ', '_')}_reviews.csv\"\n",
        "            with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                fieldnames = ['Product URL', 'User name', 'Rating', 'Review_date', 'Review_title', 'Review_content', 'Review_size']\n",
        "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                writer.writerows(reviews)\n",
        "            time.sleep(20)\n",
        "    else:\n",
        "        print(f\"Failed to fetch reviews for {product_url}\")\n",
        "\n",
        "# Read the main CSV file with product URLs\n",
        "data = pd.read_csv('amazon_products.csv')\n",
        "\n",
        "# Loop through each product URL and scrape reviews to create separate CSV files\n",
        "for index, row in data.iterrows():\n",
        "    print(index)\n",
        "    product_url = row['Product URL']\n",
        "    print(product_url)\n",
        "    scrape_reviews_to_csv(product_url, row)\n",
        "\n"
      ],
      "metadata": {
        "id": "k9yrthH3SHEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ccb0ca-d511-42dd-e1bb-afc534e30587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "https://www.amazon.in/Samsung-Galaxy-Phantom-Black-Storage/dp/B0BY8PRH1Y/ref=ice_ac_b_dpb?keywords=Galaxy+S23&qid=1702290066&sr=8-1\n",
            "1\n",
            "https://www.amazon.in/Samsung-Galaxy-Cream-256GB-Storage/dp/B0BTYVTMT6/ref=sr_1_2?keywords=Galaxy+S23&qid=1702290066&sr=8-2\n",
            "2\n",
            "https://www.amazon.in/Samsung-Galaxy-Lavender-256GB-Storage/dp/B0BRSL2XWP/ref=sr_1_3?keywords=Galaxy+S23&qid=1702290066&sr=8-3\n",
            "3\n",
            "https://www.amazon.in/Samsung-Galaxy-Cream-128GB-Storage/dp/B0BT9F9SJJ/ref=sr_1_4?keywords=Galaxy+S23&qid=1702290066&sr=8-4\n",
            "4\n",
            "https://www.amazon.in/Samsung-Galaxy-Green-256GB-Storage/dp/B0BTYX1RP4/ref=sr_1_5?keywords=Galaxy+S23&qid=1702290066&sr=8-5\n",
            "5\n",
            "https://www.amazon.in/Samsung-Galaxy-Green-256GB-Storage/dp/B0BT9DVZLZ/ref=sr_1_6?keywords=Galaxy+S23&qid=1702290066&sr=8-6\n",
            "6\n",
            "https://www.amazon.in/SAMSUNG-Galaxy-S23-Graphite-Storage/dp/B0CJXQX3MB/ref=sr_1_7?keywords=Galaxy+S23&qid=1702290066&sr=8-7\n",
            "7\n",
            "https://www.amazon.in/Samsung-Galaxy-Ultra-Green-Storage/dp/B0BTYWFXKC/ref=sr_1_8?keywords=Galaxy+S23&qid=1702290066&sr=8-8\n",
            "8\n",
            "https://www.amazon.in/Samsung-Galaxy-Ultra-Phantom-Storage/dp/B0BT9FF5FL/ref=sr_1_9?keywords=Galaxy+S23&qid=1702290066&sr=8-9\n",
            "9\n",
            "https://www.amazon.in/Samsung-Galaxy-Ultra-Phantom-Storage/dp/B0BTWQZBGP/ref=sr_1_10?keywords=Galaxy+S23&qid=1702290066&sr=8-10\n",
            "10\n",
            "https://www.amazon.in/Samsung-Galaxy-Green-128GB-Storage/dp/B0BY8PYVS6/ref=sr_1_11?keywords=Galaxy+S23&qid=1702290066&sr=8-11\n",
            "11\n",
            "https://www.amazon.in/SAMSUNG-Galaxy-Ultra-Green-Storage/dp/B0BTYSCDVS/ref=sr_1_12?keywords=Galaxy+S23&qid=1702290066&sr=8-12\n",
            "12\n",
            "https://www.amazon.in/Samsung-Galaxy-Mint-128GB-Storage/dp/B0CJ4S724M/ref=sr_1_13?keywords=Galaxy+S23&qid=1702290066&sr=8-13\n",
            "13\n",
            "https://www.amazon.in/SAMSUNG-Galaxy-S23-Mint-Storage/dp/B0CJXPYJC3/ref=sr_1_14?keywords=Galaxy+S23&qid=1702290066&sr=8-14\n",
            "14\n",
            "https://www.amazon.in/SAMSUNG-Galaxy-S23-Purple-Storage/dp/B0CJXQV9R2/ref=sr_1_15?keywords=Galaxy+S23&qid=1702290066&sr=8-15\n",
            "15\n",
            "https://www.amazon.in/SAMSUNG-Galaxy-S23-Purple-Storage/dp/B0CJXPJJGR/ref=sr_1_16?keywords=Galaxy+S23&qid=1702290066&sr=8-16\n",
            "16\n",
            "https://www.amazon.in/Samsung-Galaxy-Purple-128GB-Storage/dp/B0CJ4TFLTT/ref=sr_1_17?keywords=Galaxy+S23&qid=1702290088&sr=8-17\n",
            "17\n",
            "https://www.amazon.in/Samsung-Galaxy-Graphite-256GB-Storage/dp/B0CJ4TSNWD/ref=sr_1_18?keywords=Galaxy+S23&qid=1702290088&sr=8-18\n",
            "18\n",
            "https://www.amazon.in/Samsung-Galaxy-Graphite-128GB-Storage/dp/B0CJ4SCY75/ref=sr_1_19?keywords=Galaxy+S23&qid=1702290088&sr=8-19\n",
            "19\n",
            "https://www.amazon.in/SAMSUNG-Galaxy-S23-FE-Storage/dp/B0CLVR8HYV/ref=sr_1_20?keywords=Galaxy+S23&qid=1702290088&sr=8-20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Create an empty DataFrame to store all reviews\n",
        "all_reviews = pd.DataFrame()\n",
        "\n",
        "# Directory where the individual review CSV files are stored\n",
        "reviews_directory = '/content'  # Replace with the directory containing individual CSV files\n",
        "\n",
        "# Loop through each file in the directory\n",
        "for filename in os.listdir(reviews_directory):\n",
        "    if filename.endswith(\"_reviews.csv\"):\n",
        "        # Read individual CSV file\n",
        "        file_path = os.path.join(reviews_directory, filename)\n",
        "        print(file_path)\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Extract product name from the file name\n",
        "        product_name = filename.split('_reviews.csv')[0].replace('_', ' ')\n",
        "\n",
        "        # Add 'Product Name' column to the DataFrame\n",
        "        df['Product Name'] = product_name\n",
        "\n",
        "        # Append reviews to the combined DataFrame\n",
        "        all_reviews = all_reviews.append(df, ignore_index=True)\n",
        "\n",
        "# Save combined reviews to a new CSV file\n",
        "all_reviews.to_csv('combined.csv', index=False)\n"
      ],
      "metadata": {
        "id": "8VDgjuhUdYDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ca7d60-7f07-4a54-a987-f8368a25410d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Samsung_Galaxy_S23_5G_(Green,_8GB_Ram,_256GB_Storage)_reviews.csv\n",
            "/content/Samsung_Galaxy_S23_FE_5G_(Purple,_8GB,_128GB_Storage)_reviews.csv\n",
            "/content/Samsung_Galaxy_S23_FE_5G_(Graphite,_8GB,_256GB_Storage)_reviews.csv\n",
            "/content/SAMSUNG_Galaxy_S23_FE_5G_(Mint_256_GB_Storage)_(8_GB_RAM)_reviews.csv\n",
            "/content/Samsung_Galaxy_S23_5G_(Cream,_8GB,_128GB_Storage)_reviews.csv\n",
            "/content/Samsung_Galaxy_S23_5G_(Green,_8GB,_128GB_Storage)_reviews.csv\n",
            "/content/Samsung_Galaxy_S23_5G_(Green,_8GB,_256GB_Storage)_reviews.csv\n",
            "/content/Samsung_Galaxy_S23_Ultra_5G_(Phantom_Black,_12GB,_512GB_Storage)_reviews.csv\n",
            "/content/Samsung_Galaxy_S23_5G_(Cream,_8GB_Ram,_256GB_Storage)_reviews.csv\n",
            "/content/SAMSUNG_Galaxy_S23_Ultra_5G_(Green,_12GB_RAM,_512GB_Storage)_reviews.csv\n",
            "/content/Samsung_Galaxy_S23_FE_5G_(Graphite,_8GB,_128GB_Storage)_reviews.csv\n",
            "/content/SAMSUNG_Galaxy_S23_FE_5G_(Graphite_128_GB_Storage)_(8_GB_RAM)_reviews.csv\n",
            "/content/Samsung_Galaxy_S23_5G_(Lavender,_8GB,_256GB_Storage)_reviews.csv\n",
            "/content/Samsung_Galaxy_S23_5G_(Phantom_Black,_8GB,_128GB_Storage)_reviews.csv\n",
            "/content/Samsung_Galaxy_S23_Ultra_5G_(Phantom_Black,_12GB,_256GB_Storage)_reviews.csv\n",
            "/content/SAMSUNG_Galaxy_S23_FE_5G_(Purple_256_GB_Storage)_(8_GB_RAM)_reviews.csv\n",
            "/content/Samsung_Galaxy_S23_FE_5G_(Mint,_8GB,_128GB_Storage)_reviews.csv\n",
            "/content/SAMSUNG_Galaxy_S23_FE_5G_(Mint_128_GB_Storage)_(8_GB_RAM)_reviews.csv\n",
            "/content/Samsung_Galaxy_S23_Ultra_5G_(Green,_12GB,_256GB_Storage)_reviews.csv\n",
            "/content/SAMSUNG_Galaxy_S23_FE_5G_(Purple_128_GB_Storage)_(8_GB_RAM)_reviews.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n",
            "<ipython-input-5-049b37f6f0e4>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  all_reviews = all_reviews.append(df, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Read the combined reviews CSV file\n",
        "reviews_data = pd.read_csv('combined.csv')\n",
        "\n",
        "# Function for data preprocessing\n",
        "def preprocess_review(row):\n",
        "    sentence = row['Review_content']\n",
        "    product_name = row['Product Name']\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(sentence)\n",
        "\n",
        "    # Removing Stop Words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # POS Tagging\n",
        "    pos_tags = pos_tag(filtered_tokens)\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "    return {\n",
        "        'Product Name': product_name,\n",
        "        'Original Sentence': sentence,\n",
        "        'Tokens': tokens,\n",
        "        'Filtered Tokens (after removing stop words)': filtered_tokens,\n",
        "        'POS Tags': pos_tags,\n",
        "        'Stemmed Words': stemmed_words\n",
        "    }\n",
        "\n",
        "# Apply preprocessing function to each review\n",
        "processed_reviews = reviews_data.apply(preprocess_review, axis=1)\n",
        "\n",
        "# Convert list of dictionaries to a DataFrame\n",
        "processed_reviews_df = pd.DataFrame(processed_reviews.tolist())\n",
        "\n",
        "# Reorder columns for better readability\n",
        "processed_reviews_df = processed_reviews_df[['Product Name', 'Original Sentence', 'Tokens', 'Filtered Tokens (after removing stop words)', 'POS Tags', 'Stemmed Words']]\n",
        "\n",
        "# Save processed reviews to a new CSV file\n",
        "processed_reviews_df.to_csv('processed.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "emuNalqBxok8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f4699d-95cc-4c5b-9e34-5b207611f96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Read the combined reviews CSV file\n",
        "reviews_data = pd.read_csv('combined.csv')\n",
        "\n",
        "# Function for negative polarity identification\n",
        "def identify_negative_polarities(sentence):\n",
        "    # Tokenize sentence\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "    # Get POS tags for the sentence\n",
        "    tagged_words = nltk.pos_tag(tokens)\n",
        "\n",
        "    negative_prefixes = ['not', 'no', 'never', 'none', 'nobody', 'nowhere', 'nothing', 'neither', 'nor', 'hardly',\n",
        "                         'scarcely', 'barely']\n",
        "\n",
        "    NOA_phrases = []\n",
        "    NOV_phrases = []\n",
        "\n",
        "    for i in range(len(tagged_words) - 1):\n",
        "        word, tag = tagged_words[i]\n",
        "        next_word, next_tag = tagged_words[i + 1] if i + 1 < len(tagged_words) else (None, None)\n",
        "\n",
        "        if word.lower() in negative_prefixes:\n",
        "            if next_tag in ['JJ', 'JJR', 'JJS']:  # Adjective tags\n",
        "                NOA_phrases.append((word, next_word))\n",
        "            elif next_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:  # Verb tags\n",
        "                NOV_phrases.append((word, next_word))\n",
        "\n",
        "            if next_word and next_tag in ['JJ', 'JJR', 'JJS']:\n",
        "                third_word, third_tag = tagged_words[i + 2] if i + 2 < len(tagged_words) else (None, None)\n",
        "                if third_tag in ['JJ', 'JJR', 'JJS']:\n",
        "                    NOA_phrases.append((word, next_word, third_word))\n",
        "\n",
        "            if next_word and next_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
        "                third_word, third_tag = tagged_words[i + 2] if i + 2 < len(tagged_words) else (None, None)\n",
        "                if third_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
        "                    NOV_phrases.append((word, next_word, third_word))\n",
        "\n",
        "    return NOA_phrases, NOV_phrases\n",
        "\n",
        "# Apply negative polarity identification to each review\n",
        "reviews_data['NOA_Phrases'] = reviews_data['Review_content'].apply(lambda x: identify_negative_polarities(str(x))[0])\n",
        "reviews_data['NOV_Phrases'] = reviews_data['Review_content'].apply(lambda x: identify_negative_polarities(str(x))[1])\n",
        "\n",
        "# Save the updated reviews to a new CSV file\n",
        "reviews_data.to_csv('reviews_with_negative_polarities.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Xe1STMO9yanB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75606edc-e695-45cd-d4a2-e7d9bc5b39ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Read the combined reviews CSV file\n",
        "reviews_data = pd.read_csv('combined.csv')\n",
        "\n",
        "# Function for negative polarity identification\n",
        "def identify_negative_polarities(row):\n",
        "    sentence = str(row['Review_content'])\n",
        "    product_name = row['Product Name']\n",
        "\n",
        "    # Tokenize sentence\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "    # Get POS tags for the sentence\n",
        "    tagged_words = nltk.pos_tag(tokens)\n",
        "\n",
        "    negative_prefixes = ['not', 'no', 'never', 'none', 'nobody', 'nowhere', 'nothing', 'neither', 'nor', 'hardly',\n",
        "                         'scarcely', 'barely']\n",
        "\n",
        "    NOA_phrases = []\n",
        "    NOV_phrases = []\n",
        "\n",
        "    for i in range(len(tagged_words) - 1):\n",
        "        word, tag = tagged_words[i]\n",
        "        next_word, next_tag = tagged_words[i + 1] if i + 1 < len(tagged_words) else (None, None)\n",
        "\n",
        "        if word.lower() in negative_prefixes:\n",
        "            if next_tag in ['JJ', 'JJR', 'JJS']:  # Adjective tags\n",
        "                NOA_phrases.append((word, next_word, product_name))\n",
        "            elif next_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:  # Verb tags\n",
        "                NOV_phrases.append((word, next_word, product_name))\n",
        "\n",
        "            if next_word and next_tag in ['JJ', 'JJR', 'JJS']:\n",
        "                third_word, third_tag = tagged_words[i + 2] if i + 2 < len(tagged_words) else (None, None)\n",
        "                if third_tag in ['JJ', 'JJR', 'JJS']:\n",
        "                    NOA_phrases.append((word, next_word, third_word, product_name))\n",
        "\n",
        "            if next_word and next_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
        "                third_word, third_tag = tagged_words[i + 2] if i + 2 < len(tagged_words) else (None, None)\n",
        "                if third_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
        "                    NOV_phrases.append((word, next_word, third_word, product_name))\n",
        "\n",
        "    return NOA_phrases, NOV_phrases\n",
        "\n",
        "# Apply negative polarity identification to each review\n",
        "reviews_data[['NOA_Phrases', 'NOV_Phrases']] = reviews_data.apply(identify_negative_polarities, axis=1, result_type='expand')\n",
        "\n",
        "# Save the updated reviews to a new CSV file\n",
        "reviews_data.to_csv('reviews_with_negative_polarities.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Osw9x669z76G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26862d85-c84b-45a5-d8c4-6e0b04ec91a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "# Replace 'your_dataset.csv' with the path to your dataset\n",
        "dataset = pd.read_csv('reviews_with_negative_polarities.csv')\n",
        "\n",
        "# Assuming your dataset has a 'Rating' column containing ratings in the format '4.0 out of 5 stars'\n",
        "\n",
        "# Function to extract numeric value from the 'Rating' column\n",
        "def extract_numeric_rating(rating):\n",
        "    try:\n",
        "        return float(rating.split()[0])  # Extract the first part before the space and convert to float\n",
        "    except:\n",
        "        return None  # Return None for cases where the format doesn't match\n",
        "\n",
        "# Apply the function to create a new 'Numeric_Rating' column\n",
        "dataset['Numeric_Rating'] = dataset['Rating'].apply(extract_numeric_rating)\n",
        "dataset.to_csv('reviews_with_negative_polarities.csv', index=False)\n",
        "# Display the updated dataset\n",
        "print(dataset[['Rating', 'Numeric_Rating']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGdW1o354aA-",
        "outputId": "6f51c740-28c5-4f7b-f18c-9fbcb2b84bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Rating  Numeric_Rating\n",
            "0    5.0 out of 5 stars             5.0\n",
            "1    5.0 out of 5 stars             5.0\n",
            "2    4.0 out of 5 stars             4.0\n",
            "3    5.0 out of 5 stars             5.0\n",
            "4    4.0 out of 5 stars             4.0\n",
            "..                  ...             ...\n",
            "110                 NaN             NaN\n",
            "111  4.0 out of 5 stars             4.0\n",
            "112  1.0 out of 5 stars             1.0\n",
            "113  4.0 out of 5 stars             4.0\n",
            "114  1.0 out of 5 stars             1.0\n",
            "\n",
            "[115 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import math\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load your preprocessed dataset for sentiment analysis\n",
        "# Replace 'your_dataset.csv' with the path to your preprocessed dataset\n",
        "dataset = pd.read_csv('/content/reviews_with_negative_polarities.csv')\n",
        "\n",
        "# Assuming your dataset has 'Review_content' and 'Rating' columns\n",
        "# Adjust column names accordingly if they differ in your dataset\n",
        "\n",
        "# Define a function to categorize sentiment based on rating\n",
        "def categorize_sentiment(rating):\n",
        "    if math.isnan(rating):\n",
        "        return 'Neutral'  # Return Neutral if rating is NaN\n",
        "\n",
        "    rating_value = int(rating)\n",
        "    if rating_value > 3:\n",
        "        return 'Positive'\n",
        "    elif rating_value < 3:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'  # Rating of 3 will also be considered as Neutral\n",
        "\n",
        "\n",
        "# Create 'Sentiment' column based on 'Rating'\n",
        "dataset['Sentiment'] = dataset['Numeric_Rating'].apply(categorize_sentiment)\n",
        "\n",
        "# Tokenizing and vectorizing the text data\n",
        "tfidf = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english')\n",
        "X = tfidf.fit_transform(dataset['Review_content'])\n",
        "y = dataset['Sentiment']  # 'Sentiment' column is now the target variable\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize SVM classifier\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "\n",
        "# Train the SVM model\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = svm_classifier.score(X_test, y_test)\n",
        "print(f\"Accuracy of the SVM model: {accuracy}\")\n",
        "dataset.to_csv('/content/reviews_with_negative_polarities.csv', index=False)\n",
        "# Serialize the trained model\n",
        "import pickle\n",
        "# Replace 'svm_model.pkl' with the desired file path and name to save the trained model\n",
        "with open('svm_model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(svm_classifier, model_file)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_2hV0FY0s5F",
        "outputId": "0aa4b735-42e1-4d36-82e6-c9a28f67556b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the SVM model: 0.9130434782608695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Define a function to perform sentiment analysis for products containing specific keywords in the product name\n",
        "def find_best_product_by_keyword(keyword):\n",
        "    # Filter the dataset for product names containing the specified keyword\n",
        "    filtered_data = dataset[dataset['Product Name'].str.contains(keyword, case=False, na=False)]\n",
        "\n",
        "    if filtered_data.empty:\n",
        "        print(f\"No products found containing the keyword: '{keyword}'\")\n",
        "        return\n",
        "\n",
        "    # Predict sentiments for the filtered products\n",
        "    X_filtered = tfidf.transform(filtered_data['Review_content'])  # Assuming 'Review_content' is used for prediction\n",
        "    y_filtered_predicted = svm_classifier.predict(X_filtered)  # Predict sentiments for the filtered products\n",
        "\n",
        "    # Add predicted sentiments to the filtered dataset\n",
        "    filtered_data['Predicted_Sentiment'] = y_filtered_predicted\n",
        "\n",
        "    # Aggregate sentiments for the filtered products\n",
        "    sentiment_counts = filtered_data['Predicted_Sentiment'].value_counts()\n",
        "\n",
        "    print(f\"Sentiment analysis results for products containing the keyword '{keyword}':\")\n",
        "    print(sentiment_counts)\n",
        "\n",
        "    # Calculate the total number of positive sentiments for each product containing the keyword\n",
        "    filtered_sentiment_counts = filtered_data.groupby('Product Name')['Predicted_Sentiment'].value_counts().unstack().fillna(0)\n",
        "    filtered_sentiment_counts['Total_Positive'] = filtered_sentiment_counts['Positive']\n",
        "\n",
        "    # Identify the product with the most positive sentiments among those containing the keyword\n",
        "    best_product_filtered = filtered_sentiment_counts['Total_Positive'].idxmax()\n",
        "    #best_product_filtered = \"Samsung \"+ best_product_filtered\n",
        "    best_product_name = best_product_filtered\n",
        "    best_product_url = filtered_data.loc[filtered_data['Product Name'] == best_product_name, 'Product URL'].iloc[0]\n",
        "    if pd.notnull(best_product_url):\n",
        "        print(f\"The URL for the best product '{best_product_name}' is: {best_product_url}\")\n",
        "    else:\n",
        "        print(f\"URL not found for the best product '{best_product_name}'\")\n",
        "\n",
        "    print(f\"The best product among those containing '{keyword}' is: {best_product_name}\")\n",
        "\n",
        "    #print(f\"Product URL: {best_product_url}\")\n",
        "    #print(filtered_sentiment_counts['Total_Positive'])\n",
        "\n",
        "# Search for specific keywords and find the best product among products containing those keywords\n",
        "searched_keyword = 'Galaxy S23'  # Replace with the keyword you want to search for in product names\n",
        "find_best_product_by_keyword(searched_keyword)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0xPflE-D8bZ",
        "outputId": "ef1136b0-9600-4f80-8c4f-a40744ce004e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis results for products containing the keyword 'Galaxy S23':\n",
            "Positive    72\n",
            "Negative    24\n",
            "Neutral     19\n",
            "Name: Predicted_Sentiment, dtype: int64\n",
            "The URL for the best product 'Samsung Galaxy S23 Ultra 5G (Phantom Black, 12GB, 256GB Storage)' is: https://www.amazon.in/Samsung-Galaxy-Ultra-Phantom-Storage/dp/B0BTWQZBGP/ref=sr_1_10?keywords=Galaxy+S23&qid=1702290066&sr=8-10\n",
            "The best product among those containing 'Galaxy S23' is: Samsung Galaxy S23 Ultra 5G (Phantom Black, 12GB, 256GB Storage)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Assuming you have trained a model 'svm_classifier' and have your test set 'X_test'\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Now, you can compute various evaluation metrics using the predicted values 'y_pred' and the true labels 'y_test'\n",
        "\n",
        "# Compute the accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Compute the precision (micro, macro, weighted or None based on your requirement)\n",
        "pre_micro = precision_score(y_test, y_pred, average='micro')\n",
        "pre_macro = precision_score(y_test, y_pred, average='macro')\n",
        "pre_weighted = precision_score(y_test, y_pred, average='weighted')\n",
        "pre_none = precision_score(y_test, y_pred, average=None)  # For multiclass, returns precision for each class\n",
        "\n",
        "# Compute the recall (micro, macro, weighted or None based on your requirement)\n",
        "rec_micro = recall_score(y_test, y_pred, average='micro')\n",
        "rec_macro = recall_score(y_test, y_pred, average='macro')\n",
        "rec_weighted = recall_score(y_test, y_pred, average='weighted')\n",
        "rec_none = recall_score(y_test, y_pred, average=None)  # For multiclass, returns recall for each class\n",
        "\n",
        "# Compute the F1 score (micro, macro, weighted or None based on your requirement)\n",
        "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
        "f1_none = f1_score(y_test, y_pred, average=None)  # For multiclass, returns F1 score for each class\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ef7wk560EAnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\", acc)\n",
        "\n",
        "print(\"Precision (Micro):\", pre_micro)\n",
        "print(\"Precision (Macro):\", pre_macro)\n",
        "print(\"Precision (Weighted):\", pre_weighted)\n",
        "print(\"Precision (None):\", pre_none)\n",
        "\n",
        "print(\"Recall (Micro):\", rec_micro)\n",
        "print(\"Recall (Macro):\", rec_macro)\n",
        "print(\"Recall (Weighted):\", rec_weighted)\n",
        "print(\"Recall (None):\", rec_none)\n",
        "\n",
        "print(\"F1 Score (Micro):\", f1_micro)\n",
        "print(\"F1 Score (Macro):\", f1_macro)\n",
        "print(\"F1 Score (Weighted):\", f1_weighted)\n",
        "print(\"F1 Score (None):\", f1_none)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3gTxJEMKjbz",
        "outputId": "dd071954-d534-4a6d-c006-685595b36a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9130434782608695\n",
            "Precision (Micro): 0.9130434782608695\n",
            "Precision (Macro): 0.9607843137254902\n",
            "Precision (Weighted): 0.9232736572890026\n",
            "Precision (None): [1.         1.         0.88235294]\n",
            "Recall (Micro): 0.9130434782608695\n",
            "Recall (Macro): 0.7777777777777778\n",
            "Recall (Weighted): 0.9130434782608695\n",
            "Recall (None): [0.83333333 0.5        1.        ]\n",
            "F1 Score (Micro): 0.9130434782608695\n",
            "F1 Score (Macro): 0.8377525252525252\n",
            "F1 Score (Weighted): 0.906538208168643\n",
            "F1 Score (None): [0.90909091 0.66666667 0.9375    ]\n",
            "Confusion Matrix:\n",
            " [[ 5  0  1]\n",
            " [ 0  1  1]\n",
            " [ 0  0 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result Analysis\n",
        "\n",
        "#### Accuracy\n",
        "The model achieved an accuracy of approximately 91.3%, signifying the overall correctness in classifying sentiments across all classes.\n",
        "\n",
        "#### Precision\n",
        "- **Micro-average Precision:** It represents the precision calculated globally by considering the total true positive, false positive, and false negative values across all classes. In this case, the micro-precision is also 91.3%.\n",
        "- **Macro-average Precision:** It computes the average precision for each class without considering class imbalance. The macro-precision is approximately 96.1%, suggesting good precision across classes.\n",
        "- **Weighted-average Precision:** It calculates the precision for each class, considering the number of samples in each class. The weighted precision here is around 92.3%.\n",
        "- **Class-specific Precision:** It indicates precision values for each class separately. The classes have precision values of 100%, 100%, and approximately 88.2%, respectively.\n",
        "\n",
        "#### Recall\n",
        "- **Micro-average Recall:** It denotes the recall calculated globally, considering total true positive, false positive, and false negative values across all classes. The micro-recall achieved is 91.3%.\n",
        "- **Macro-average Recall:** It computes the average recall for each class without considering class imbalance. The macro-recall is approximately 77.8%, indicating some variability in class-specific recall values.\n",
        "- **Weighted-average Recall:** It calculates the recall for each class, considering the number of samples in each class. The weighted recall here is 91.3%.\n",
        "- **Class-specific Recall:** The recall values for classes are approximately 83.3%, 50%, and 100%, respectively.\n",
        "\n",
        "#### F1 Score\n",
        "- **Micro-average F1 Score:** It represents the harmonic mean of precision and recall calculated globally across all classes. The micro-F1 score achieved is 91.3%.\n",
        "- **Macro-average F1 Score:** It calculates the average F1 score for each class without considering class imbalance. The macro-F1 score is approximately 83.8%.\n",
        "- **Weighted-average F1 Score:** It computes the F1 score for each class, considering the number of samples in each class. The weighted F1 score here is around 90.7%.\n",
        "- **Class-specific F1 Score:** The F1 score values for each class are approximately 90.9%, 66.7%, and 93.8%, respectively.\n",
        "\n",
        "#### Confusion Matrix\n",
        "The confusion matrix displays the model's classification results. It indicates that:\n",
        "- Class 0: 5 samples correctly predicted, 0 samples incorrectly predicted as other classes, and 1 sample misclassified.\n",
        "- Class 1: 1 sample correctly predicted for this class, 0 samples incorrectly predicted as other classes, and 1 sample misclassified.\n",
        "- Class 2: 15 samples correctly predicted for this class, with no misclassifications.\n",
        "\n",
        "### Summary\n",
        "The model demonstrates high accuracy, especially in correctly classifying samples for Class 2, while exhibiting some misclassifications for Classes 0 and 1. Further analysis and potentially fine-tuning the model could be beneficial to address misclassifications and improve performance, especially for Classes 0 and 1."
      ],
      "metadata": {
        "id": "xazY3MjqMK-N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IMfINsQaAa0t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}